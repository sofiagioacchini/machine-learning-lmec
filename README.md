# Forecasting with approximate dynamic factor models: the role of non-pervasive shocks
## Replication of the results of the paper by Matteo Luciani (2014) [1^]

In the last decade datasets of very large dimensions (Big Data) have become largely available and so have factor models become increasingly adopted by econometricians, as a popular method of dimension reduction, that allows to manage these datasets and exploit their underlying patterns. Indeed, factor models are able to simplify data by reducing it to a smaller set of essential components, i.e. the common factors ($q$). This way they present a solution to the curse of dimensionality ($n \geq T$) issue, which constitutes a problem due to the lack of degrees of freedom. In general, a factor model for a high-dimensional vector of time series is characterized by a few latent factors, which capture the co-movements of the variables, and idiosyncratic terms, capturing the idiosyncratic dynamics.

A large body of literature, including Bai & Ng (2002), Forni, Hallin, Lippi & Reichlin (2005), has shown how factor models can outperform common univariate benchmark forecasts by simply forecasting the common component (i.e., the one driven by pervasive shocks), while approximating with an autoregressive model the idiosyncratic component (i.e., the one driven by non-pervasive shocks). In other terms, traditionally, forecasts are approximated by:
 <img width="198" alt="Schermata 2023-11-02 alle 22 00 52" src="https://github.com/sofiagioacchini/machine-learning-lmec/assets/149707413/5c4c1da3-9e70-4c84-b51f-d8a7d6026681">
as if the idiosyncracies are not cross-sectionally correlated or mutually orthogonal.

Luciani (2014) notes that this approximation is contradictory since factor models are estimated under the very hypothesis of weakly cross-sectionally dynamically correlated errors, and that this may hold some implications in forecasting approximate dynamic factor models. Here lies the core of Luciani's research question. The author identifies this technical flaw in research literature on dynamic factor models and aims at weighting the technical relevance of this contradiction and its research implications. He does so by augmenting two factor models for explaining macroeconomic dynamics with a sparse model on the idiosyncratic component. Hereby, he combines factor models and Bayesian shrinkage, performed through L1 penalisation and boosting, on Xi to capture the comovement in the data and local correlation, respectively. The h-step ahead forecast is the linear projection with shrinkage onto the space spanned by the idiosyncratic components. In synthesis, his approach is similar to factor models in that it summarizes the comovement in the data with a few common
shocks, while it differs from factor models in that it considers the whole space spanned by the idiosyncratic component, rather than approximating it by the present and past values of the variable-specific dynamics. 

Finally, our work consisted in the reproduction of Luciani's paper and in the comparison between his models (ML1 and ML2) results and previous models presented in literature, augmented FHLR1, i.e. FHLR2, with an AR1 or the idiosyncratic component, DGR1 and DGR2 that impose no factor structure but perform forecasts through penalized regression and boosting). Practically, we worked the models in R and MatLab and compared the Mean Squared Errors (MSE) to evaluate the performances of the different models and, in particular, to evaluate whether accounting for non-pervasive shocks serves as an implementation of forecasting with approximate dynamic factor models. Lastly, we expand the comparison by adding another model, XGBoost, i.e. extreme gradient boosting; this model is an implementation of gradient boosting trees and has emerged the main challenger of deep learning approaches when it comes to structured data. Indeed, it employs regularization to prevent overfitting, can be trained on data that include missing values, so that when data is sparse an instance is classified in a default direction that is learnt from the data; this is particularly interesting when forecasting with macroeconomic data as the variables often have different historical depth. Lastly, through the feature of early stopping to define the number of trees, this algorithm reduces training time.

[1^]: Luciani, M. (2014). Forecasting with approximate dynamic factor models: the role of non-pervasive shocks. International Journal of Forecasting, 30(1), 20-29.
